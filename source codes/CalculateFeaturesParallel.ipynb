{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96777c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import scipy.spatial.distance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.neighbors import radius_neighbors_graph, kneighbors_graph\n",
    "import pickle\n",
    "\n",
    "from CloudPointsPreprocessing import * \n",
    "from FeatureConcatModel import * \n",
    "from GraphPreprocessing import *\n",
    "from PointNet import *\n",
    "from PointNetBasedGraphPoolingModel import *\n",
    "from ReportVisualization import * \n",
    "from SelfAttentionGraphPooling import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4abe695",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_global = Path(\"ModelNet10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4764e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of classes:  {'bathtub': 0, 'bed': 1, 'chair': 2, 'desk': 3, 'dresser': 4, 'monitor': 5, 'night_stand': 6, 'sofa': 7, 'table': 8, 'toilet': 9}\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n",
    "    classes = {folder: i for i, folder in enumerate(folders)}\n",
    "    print(\"Name of classes: \", classes)\n",
    "\n",
    "    with open(path/\"bed/train/bed_0001.off\", 'r') as file:\n",
    "        if 'OFF' != file.readline().strip():\n",
    "            raise('Not a valid OFF header')\n",
    "        n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "        verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
    "        faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
    "    return verts, faces,classes\n",
    "\n",
    "verts, faces, classes = load_data(path_global)\n",
    "i,j,k = np.array(faces).T\n",
    "x,y,z = np.array(verts).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf6581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointSampler(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def triangle_area(self, pt1, pt2, pt3):\n",
    "        side_a = np.linalg.norm(pt1 - pt2)\n",
    "        side_b = np.linalg.norm(pt2 - pt3)\n",
    "        side_c = np.linalg.norm(pt3 - pt1)\n",
    "        s = 0.5 * ( side_a + side_b + side_c)\n",
    "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
    "\n",
    "    def sample_point(self, pt1, pt2, pt3):\n",
    "        s, t = sorted([random.random(), random.random()])\n",
    "        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n",
    "        return (f(0), f(1), f(2))\n",
    "\n",
    "\n",
    "    def __call__(self, mesh):\n",
    "        verts, faces = mesh\n",
    "        verts = np.array(verts)\n",
    "        areas = np.zeros((len(faces)))\n",
    "\n",
    "        for i in range(len(areas)):\n",
    "            areas[i] = (self.triangle_area(verts[faces[i][0]],verts[faces[i][1]],verts[faces[i][2]]))\n",
    "        sampled_faces = (random.choices(faces, weights=areas,cum_weights=None, k=self.output_size))\n",
    "        sampled_points = np.zeros((self.output_size, 3))\n",
    "        for i in range(len(sampled_faces)):\n",
    "            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]], verts[sampled_faces[i][1]],verts[sampled_faces[i][2]]))\n",
    "        return sampled_points\n",
    "\n",
    "class Normalize(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0)\n",
    "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
    "        return  norm_pointcloud\n",
    "\n",
    "class RandRotation_z(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        theta = random.random() * 2. * math.pi\n",
    "        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
    "                               [ math.sin(theta),  math.cos(theta),    0],\n",
    "                               [0,                             0,      1]])\n",
    "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
    "        return  rot_pointcloud\n",
    "\n",
    "class RandomNoise(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
    "        noisy_pointcloud = pointcloud + noise\n",
    "        return  noisy_pointcloud\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        return torch.from_numpy(pointcloud)\n",
    "\n",
    "\n",
    "def DefaultTransforms():\n",
    "    return transforms.Compose([ PointSampler(1024), Normalize(),ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2420e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_features(point_cloud,N=6):\n",
    "    connection_matrix = kneighbors_graph(point_cloud,N,mode='distance').toarray()\n",
    "    graph = nx.DiGraph()\n",
    "    X,Y,Z = {},{},{}\n",
    "    for node in range(connection_matrix.shape[0]):\n",
    "        for neighbor in range(connection_matrix.shape[1]):\n",
    "            if neighbor == node:\n",
    "                graph.add_edge(node,neighbor,weight=0)\n",
    "                continue\n",
    "            if connection_matrix[node][neighbor] == 0 :\n",
    "                continue\n",
    "            graph.add_edge(neighbor,node)\n",
    "            graph[neighbor][node][\"weight\"] = connection_matrix[node][neighbor]\n",
    "        X[node] = np.float64(point_cloud[node][0])\n",
    "        Y[node] = np.float64(point_cloud[node][1])\n",
    "        Z[node] = np.float64(point_cloud[node][2])\n",
    "\n",
    "    features = [X,Y,Z]\n",
    "    features_name = [\"X\",\"Y\",\"Z\"]\n",
    "\n",
    "    betweenness_centrality = nx.betweenness_centrality(graph,weight=\"weight\")\n",
    "    features.append(betweenness_centrality)\n",
    "    features_name.append(\"betweenness_centrality\")\n",
    "\n",
    "    katz_centrality = nx.katz_centrality(graph,weight=\"weight\")\n",
    "    features.append(katz_centrality)\n",
    "    features_name.append(\"katz_centrality\")\n",
    "\n",
    "    closeness_centrality = nx.closeness_centrality(graph,distance=\"weight\",)\n",
    "    features.append(closeness_centrality)\n",
    "    features_name.append(\"closeness_centrality\")\n",
    "\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(graph,weight=\"weight\",max_iter=100,tol=1e-3)\n",
    "    features.append(eigenvector_centrality)\n",
    "    features_name.append(\"eigenvector_centrality\")\n",
    "\n",
    "    harmonic_centrality = nx.harmonic_centrality(graph,distance=\"weight\",)\n",
    "    features.append(harmonic_centrality)\n",
    "    features_name.append(\"harmonic_centrality\")\n",
    "\n",
    "    load_centrality = nx.load_centrality(graph,weight=\"weight\")\n",
    "    features.append(load_centrality)\n",
    "    features_name.append(\"load_centrality\")\n",
    "\n",
    "    pagerank = nx.pagerank(graph,weight='weight')\n",
    "    features.append(pagerank)\n",
    "    features_name.append(\"pagerank\")\n",
    "\n",
    "    for idx in range(len(features)):\n",
    "        nx.set_node_attributes(graph,features[idx],features_name[idx])\n",
    "\n",
    "    nodes = nx.nodes(graph)\n",
    "    features = []\n",
    "    for node_indx in range(len(nodes)): \n",
    "        features.append(np.array(list(nodes[node_indx].values())))\n",
    "    features = np.array(features)\n",
    "    \n",
    "    features[:,3:] = features[:,3:] - np.mean(features[:,3:], axis=0)\n",
    "    features[:,3:] /= np.max(np.linalg.norm(features[:,3:], axis=1))\n",
    "    \n",
    "    edge_list = np.array(nx.edges(graph))\n",
    "    \n",
    "    return torch.from_numpy(features),torch.from_numpy(edge_list), graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffcb1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudData(Dataset):\n",
    "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=DefaultTransforms(),force_to_cal = False):\n",
    "        self.root_dir = root_dir\n",
    "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
    "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
    "        self.transforms = transform if not valid else DefaultTransforms()\n",
    "        self.valid = valid\n",
    "        self.files = []\n",
    "        self.force_to_cal = force_to_cal\n",
    "        for category in self.classes.keys():\n",
    "            new_dir = root_dir/Path(category)/folder\n",
    "            for file in os.listdir(new_dir):\n",
    "                if file.endswith('.off'):\n",
    "                    sample = {}\n",
    "                    sample['pcd_path'] = new_dir/file\n",
    "                    sample['category'] = category\n",
    "                    self.files.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def read_off(self,file):\n",
    "        if 'OFF' != file.readline().strip():\n",
    "            raise('Not a valid OFF header')\n",
    "        n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "        verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
    "        faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
    "        return verts, faces\n",
    "\n",
    "    def __preproc__(self, file):\n",
    "        verts, faces = self.read_off(file)\n",
    "        if self.transforms:\n",
    "            pointcloud = self.transforms((verts, faces))\n",
    "        return pointcloud\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pcd_path = self.files[idx]['pcd_path']\n",
    "        category = self.files[idx]['category']\n",
    "        name = str(pcd_path).split(\"/\")[-1].split(\".\")[0]\n",
    "        \n",
    "        pointcloud_path = str(pcd_path).replace(name+\".off\",name+\"_pointcloud.npz\")\n",
    "        graph_feature_path = str(pcd_path).replace(name+\".off\",name+\"_graph_features.npz\")\n",
    "        graph_edge_list_path = str(pcd_path).replace(name+\".off\",name+\"_graph_edge_list.npz\")\n",
    "        graph_path = str(pcd_path).replace(name+\".off\",name+\"_graph.pickle\")\n",
    "        \n",
    "        \n",
    "        if not (os.path.exists(pointcloud_path) and os.path.exists(graph_feature_path)) or self.force_to_cal:\n",
    "            with open(pcd_path, 'r') as f:\n",
    "                pointcloud = self.__preproc__(f)\n",
    "                graph_features,edge_list,graph = get_graph_features(pointcloud)\n",
    "            \n",
    "                pointcloud_np = pointcloud.numpy()\n",
    "                graph_features_np = graph_features.numpy()\n",
    "                edge_list_np = edge_list.numpy()\n",
    "            \n",
    "                np.savez_compressed(pointcloud_path, pointcloud_np)\n",
    "                np.savez_compressed(graph_feature_path, graph_features_np)\n",
    "                np.savez_compressed(graph_edge_list_path, edge_list_np)\n",
    "                \n",
    "                with open(graph_path, 'wb') as handle:\n",
    "                    pickle.dump(graph, handle)\n",
    "              \n",
    "        else:\n",
    "                pointcloud = torch.from_numpy(np.load(pointcloud_path)[\"arr_0\"])\n",
    "                graph_features = torch.from_numpy(np.load(graph_feature_path)[\"arr_0\"])\n",
    "                edge_list = torch.from_numpy(np.load(graph_edge_list_path)[\"arr_0\"])\n",
    "                \n",
    "                with open(graph_path, 'rb') as handle:\n",
    "                    graph = pickle.load(handle)\n",
    "                \n",
    "        return {'pointcloud': pointcloud,\"edge_list\":edge_list,\"graph\":graph, 'category': self.classes[category],\n",
    "                'graph_features': graph_features}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 162255.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 48.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! -> 2\n",
      "Done! -> 3\n",
      "Done! -> 19\n",
      "Done! -> 5\n",
      "Done! -> 13\n",
      "Done! -> 14\n",
      "Done! -> 4\n",
      "Done! -> 8\n",
      "Done! -> 12\n",
      "Done! -> 16\n",
      "Done! -> 6\n",
      "Done! -> 10\n",
      "Done! -> 9\n",
      "Done! -> 17\n",
      "Done! -> 15\n",
      "Done! -> 1\n",
      "Done! -> 11\n",
      "Done! -> 7\n",
      "Done! -> 18\n",
      "Done! -> 0\n",
      "Done! -> 23\n",
      "Done! -> 28\n",
      "Done! -> 39\n",
      "Done! -> 24\n",
      "Done! -> 36\n",
      "Done! -> 33\n",
      "Done! -> 29\n",
      "Done! -> 30\n",
      "Done! -> 26\n",
      "Done! -> 25\n",
      "Done! -> 22\n",
      "Done! -> 35\n",
      "Done! -> 32\n",
      "Done! -> 21\n",
      "Done! -> 31\n",
      "Done! -> 34\n",
      "Done! -> 37\n",
      "Done! -> 20\n",
      "Done! -> 27\n",
      "Done! -> 43\n",
      "Done! -> 56\n",
      "Done! -> 59\n",
      "Done! -> 53\n",
      "Done! -> 44\n",
      "Done! -> 50\n",
      "Done! -> 49\n",
      "Done! -> 48\n",
      "Done! -> 42\n",
      "Done! -> 45\n",
      "Done! -> 51\n",
      "Done! -> 46\n",
      "Done! -> 55\n",
      "Done! -> 41\n",
      "Done! -> 38\n",
      "Done! -> 54\n",
      "Done! -> 57\n",
      "Done! -> 52\n",
      "Done! -> 40\n",
      "Done! -> 47\n",
      "Done! -> 79\n",
      "Done! -> 70\n",
      "Done! -> 73\n",
      "Done! -> 63\n",
      "Done! -> 64\n",
      "Done! -> 68\n",
      "Done! -> 65\n",
      "Done! -> 62\n",
      "Done! -> 76\n",
      "Done! -> 66\n",
      "Done! -> 61\n",
      "Done! -> 75\n",
      "Done! -> 58\n",
      "Done! -> 74\n",
      "Done! -> 71\n",
      "Done! -> 77\n",
      "Done! -> 69\n",
      "Done! -> 60\n",
      "Done! ->Done! -> 90\n",
      " 93\n",
      "Done! -> 99\n",
      "Done! -> 72\n",
      "Done! -> 88\n",
      "Done! -> 82\n",
      "Done! -> 84\n",
      "Done! -> 67\n",
      "Done! -> 85\n",
      "Done! -> 86\n",
      "Done! -> 81\n",
      "Done! -> 83\n",
      "Done! -> 96\n",
      "Done! -> 95\n",
      "Done! -> 78\n",
      "Done! -> 91\n",
      "Done! -> 94\n",
      "Done! -> 97\n",
      "Done! -> 89\n",
      "Done! -> 80\n",
      "Done! -> 110\n",
      "Done! -> 92\n",
      "Done! -> 104\n",
      "Done! -> 113\n",
      "Done! -> 102\n",
      "Done! -> 108\n",
      "Done! -> 101\n",
      "Done! -> 105\n",
      "Done! -> 103\n",
      "Done! -> 116\n",
      "Done! -> 106\n",
      "Done! -> 87\n",
      "Done! -> 98\n",
      "Done! -> 111\n",
      "Done! -> 115\n",
      "Done! -> 114\n",
      "Done! -> 124\n",
      "Done! -> 117\n",
      "Done! -> 109\n",
      "Done! -> 130\n",
      "Done! -> 112\n",
      "Done! -> 128\n",
      "Done! -> 122\n",
      "Done! ->Done! -> 119\n",
      " 100\n",
      "Done! -> 123\n",
      "Done! -> 125\n",
      "Done! -> 126\n",
      "Done! -> 107\n",
      "Done! -> 118\n",
      "Done! -> 136\n",
      "Done! -> 133\n",
      "Done! -> 135\n",
      "Done! -> 134\n",
      "Done! -> 144\n",
      "Done! -> 131\n",
      "Done! -> 137\n",
      "Done! -> 129\n",
      "Done! -> 132\n",
      "Done! -> 150\n",
      "Done! -> 142\n",
      "Done! -> 148\n",
      "Done! -> 120\n",
      "Done! -> 145\n",
      "Done! -> 127\n",
      "Done! -> 121\n",
      "Done! -> 146\n",
      "Done! -> 138\n",
      "Done! -> 143\n",
      "Done! -> 156\n",
      "Done! -> 153\n",
      "Done! -> 154\n",
      "Done! -> 139\n",
      "Done! -> 155\n",
      "Done! -> 164\n",
      "Done! -> 157\n",
      "Done! -> 151\n",
      "Done! -> 168\n",
      "Done! -> 152\n",
      "Done! -> 170\n",
      "Done! -> 149\n",
      "Done! -> 162\n",
      "Done! -> 140\n",
      "Done! -> 165\n",
      "Done! -> 147\n",
      "Done! -> 141\n",
      "Done! -> 166\n",
      "Done! -> 163\n",
      "Done! -> 176\n",
      "Done! -> 173\n",
      "Done! -> 174\n",
      "Done! -> 175\n",
      "Done! -> 159\n",
      "Done! -> 184\n",
      "Done! -> 177\n",
      "Done! -> 158\n",
      "Done! -> 171\n",
      "Done! -> 188\n",
      "Done! -> 190\n",
      "Done! -> 172\n",
      "Done! -> 182\n",
      "Done! -> 185\n",
      "Done! -> 160\n",
      "Done! -> 161\n",
      "Done! -> 167\n",
      "Done! -> 186\n",
      "Done! -> 183\n",
      "Done! -> 196\n",
      "Done! -> 194\n",
      "Done! -> 179\n",
      "Done! -> 197\n",
      "Done! -> 204\n",
      "Done! -> 193\n",
      "Done! -> 191\n",
      "Done! -> 178\n",
      "Done! -> 195\n",
      "Done! -> 192\n",
      "Done! -> 202\n",
      "Done! -> 208\n",
      "Done! -> 180\n",
      "Done! -> 205\n",
      "Done! -> 181\n",
      "Done! -> 206\n",
      "Done! -> 169\n",
      "Done! -> 210\n",
      "Done! -> 187\n",
      "Done! -> 216\n",
      "Done! -> 224\n",
      "Done! -> 211\n",
      "Done! -> 199\n",
      "Done! -> 213\n",
      "Done! -> 217\n",
      "Done! -> 215\n",
      "Done! -> 212\n",
      "Done! -> 214\n",
      "Done! -> 198\n",
      "Done! -> 222\n",
      "Done! -> 225\n",
      "Done! -> 201\n",
      "Done! -> 228\n",
      "Done! -> 189\n",
      "Done! -> 207\n",
      "Done! -> 200\n",
      "Done! -> 236\n",
      "Done! -> 230\n",
      "Done! -> 244\n",
      "Done! -> 231\n",
      "Done! -> 219\n",
      "Done! -> 237\n",
      "Done! -> 233\n",
      "Done! -> 235\n",
      "Done! -> 203\n",
      "Done! -> 232\n",
      "Done! -> 226\n",
      "Done! -> 234\n",
      "Done! -> 242\n",
      "Done! -> 218\n",
      "Done! -> 245\n",
      "Done! -> 221\n",
      "Done! -> 248\n",
      "Done! -> 209\n",
      "Done! -> 220\n",
      "Done! -> 256\n",
      "Done! -> 227\n",
      "Done! -> 250\n",
      "Done! -> 251\n",
      "Done! -> 239\n",
      "Done! -> 253\n",
      "Done! -> 257\n",
      "Done! -> 223\n",
      "Done! -> 255\n",
      "Done! -> 264\n",
      "Done! -> 252\n",
      "Done! -> 246\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(num,_cut,dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        if i%num==_cut:\n",
    "            sample = dataset[i]\n",
    "            print(\"Done! ->\",i)\n",
    "\n",
    "def handle_threads(num,dataset):\n",
    "    threads = []\n",
    "    for idx in tqdm(range(num)):\n",
    "        thread = threading.Thread(target=prepare_dataset, args=(num,idx,dataset,))\n",
    "        threads.append(thread)\n",
    "    \n",
    "    for thread in tqdm(threads):\n",
    "        thread.start()\n",
    "        \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "#custom_transforms = transforms.Compose([PointSampler(1024),Normalize(), RandRotation_z(), RandomNoise(),ToTensor()])\n",
    "#train_dataset = PointCloudData(path_global,force_to_cal=True)\n",
    "#valid_dataset = PointCloudData(path_global, valid=True, folder='test',force_to_cal=True)\n",
    "\n",
    "#handle_threads(20,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219691d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a30707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcddc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da0ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13e086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92305bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
